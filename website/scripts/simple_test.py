"""
Simple validation test for the enhanced topic extraction system.
Tests the core logic and structure without external dependencies.
"""

import json
import os
import re
from collections import defaultdict, Counter

def test_basic_functionality():
    """Test basic topic extraction functionality."""
    print("Testing Basic Topic Extraction Functionality")
    print("=" * 50)
    
    # Load the topic configuration
    script_dir = os.path.dirname(os.path.abspath(__file__))
    config_folder = os.path.join(os.path.dirname(script_dir), 'config')
    topic_config_path = os.path.join(config_folder, 'topic-extraction-data.json')
    
    print(f"Loading config from: {topic_config_path}")
    
    try:
        with open(topic_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        print(f"‚úì Successfully loaded topic configuration")
        print(f"  - Topic categories: {len(config['topicCategories'])}")
        print(f"  - Technical entities: {sum(len(v) for v in config['technicalEntities'].values())}")
        print(f"  - Stop words: {len(config['stopWords'])}")
        
    except Exception as e:
        print(f"‚úó Failed to load config: {e}")
        return False
    
    # Test content preprocessing
    sample_content = """
    # Machine Learning with Python
    
    Machine learning has revolutionized the way we approach **data science** and artificial intelligence.
    In this comprehensive guide, we'll explore the fundamentals of neural networks, deep learning algorithms,
    and their applications in computer vision and natural language processing.
    
    ```python
    import tensorflow as tf
    model = tf.keras.Sequential()
    ```
    
    Python and TensorFlow are essential tools for implementing these solutions.
    """
    
    # Basic text cleaning (mimicking the preprocess function)
    def clean_content(text):
        # Remove YAML frontmatter
        text = re.sub(r'^---\n.*?\n---\n', '', text, flags=re.DOTALL)
        # Remove HTML tags
        text = re.sub(r'<[^>]*>', '', text)
        # Remove code blocks
        text = re.sub(r'```[\s\S]*?```', '', text)
        text = re.sub(r'`[^`]*`', '', text)
        # Convert links to text
        text = re.sub(r'\[([^\]]*)\]\([^)]*\)', r'\1', text)
        # Remove markdown formatting
        text = re.sub(r'[#*_~`]', '', text)
        # Replace newlines with spaces
        text = re.sub(r'\n+', ' ', text)
        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text)
        return text.lower().strip()
    
    cleaned = clean_content(sample_content)
    print(f"\n‚úì Content preprocessing works")
    print(f"  Original length: {len(sample_content)} chars")
    print(f"  Cleaned length: {len(cleaned)} chars")
    print(f"  Sample: {cleaned[:100]}...")
    
    # Test keyword extraction (simple TF)
    words = cleaned.split()
    stop_words = set(config['stopWords'])
    filtered_words = [word for word in words if 
                     len(word) > 2 and 
                     word not in stop_words and 
                     re.match(r'^[a-z0-9-]+$', word)]
    
    word_freq = Counter(filtered_words)
    
    print(f"\n‚úì Basic keyword extraction works")
    print(f"  Total words: {len(words)}")
    print(f"  Filtered words: {len(filtered_words)}")
    print(f"  Unique words: {len(word_freq)}")
    
    # Show top keywords
    top_keywords = word_freq.most_common(10)
    if top_keywords:
        print(f"  Top keywords: {', '.join([f'{word}({count})' for word, count in top_keywords[:5]])}")
    
    # Test category mapping
    def categorize_keyword(keyword):
        for category, terms in config['topicCategories'].items():
            if any(keyword in term.lower() or term.lower() in keyword for term in terms):
                return category
        return 'general'
    
    categorized_keywords = [(word, count, categorize_keyword(word)) for word, count in top_keywords]
    
    print(f"\n‚úì Keyword categorization works")
    for word, count, category in categorized_keywords[:5]:
        print(f"  {word} -> {category}")
    
    # Test entity extraction
    normalized_content = sample_content.lower()
    entities = set()
    
    # Check technical entities
    for entity_list in config['technicalEntities'].values():
        for entity in entity_list:
            if entity.lower() in normalized_content:
                entities.add(entity)
    
    # Extract proper nouns
    proper_nouns = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', sample_content)
    for noun in proper_nouns:
        if len(noun) > 3 and noun.lower() not in stop_words:
            entities.add(noun)
    
    print(f"\n‚úì Entity extraction works")
    print(f"  Found {len(entities)} entities: {', '.join(list(entities)[:8])}")
    
    # Test topic classification
    category_scores = defaultdict(float)
    total_words = len(filtered_words)
    
    for word, count in word_freq.items():
        if count >= 2:  # Only frequent words
            tf = count / total_words
            category = categorize_keyword(word)
            if category != 'general':
                category_scores[category] += tf
    
    sorted_categories = sorted(category_scores.items(), key=lambda x: x[1], reverse=True)
    
    print(f"\n‚úì Topic classification works")
    if sorted_categories:
        primary_topic = sorted_categories[0][0]
        print(f"  Primary topic: {primary_topic}")
        print(f"  Secondary topics: {', '.join([cat for cat, _ in sorted_categories[1:4]])}")
    else:
        print("  No specific topics identified (general content)")
    
    return True

def test_file_structure():
    """Test that the enhanced system files are in place."""
    print("\nTesting File Structure")
    print("=" * 25)
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    files_to_check = [
        ('topic_discovery.py', 'Topic Discovery System'),
        ('enhanced_topic_extraction.py', 'Enhanced Topic Extractor'),
        ('create_blog_metadata.py', 'Main Metadata Script'),
        ('python-requirements.txt', 'Python Dependencies')
    ]
    
    all_exist = True
    for filename, description in files_to_check:
        filepath = os.path.join(script_dir, filename)
        if os.path.exists(filepath):
            size = os.path.getsize(filepath)
            print(f"‚úì {description}: {filename} ({size:,} bytes)")
        else:
            print(f"‚úó {description}: {filename} - Missing!")
            all_exist = False
    
    return all_exist

def main():
    """Run all tests."""
    print("Enhanced Topic Extraction System - Basic Validation")
    print("=" * 60)
    
    success = True
    
    # Test file structure
    if not test_file_structure():
        success = False
    
    # Test basic functionality
    if not test_basic_functionality():
        success = False
    
    print("\n" + "=" * 60)
    if success:
        print("üéâ Basic validation PASSED!")
        print("\nThe enhanced topic extraction system is properly set up.")
        print("\nTo use the full functionality:")
        print("1. Install dependencies: pip install -r scripts/python-requirements.txt")
        print("2. Run topic discovery: python scripts/topic_discovery.py")
        print("3. Generate metadata: python scripts/create_blog_metadata.py")
    else:
        print("‚ö†Ô∏è  Basic validation FAILED!")
        print("Please check the errors above and ensure all files are in place.")
    
    print("\nSystem Overview:")
    print("- ‚úì Static topic extraction (using predefined categories)")
    print("- ‚úì Dynamic topic discovery (ML-based, requires dependencies)")
    print("- ‚úì Hybrid classification (combines both approaches)")
    print("- ‚úì Enhanced complexity assessment")
    print("- ‚úì Improved target audience identification")
    
    return success

if __name__ == '__main__':
    main()