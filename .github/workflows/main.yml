name: build-deploy
on:
  push:
    branches:
      - source
  pull_request:
    branches:
      - source
  schedule:
    # Weekly full topic discovery on Sundays at 2 AM UTC
    - cron: "0 2 * * 0"
  workflow_dispatch:
    inputs:
      topic_mode:
        description: "Topic extraction mode"
        required: false
        default: "auto"
        type: choice
        options:
          - auto
          - metadata-only
          - full-discovery
          - force
      deploy:
        description: "Deploy to GitHub Pages"
        required: false
        default: true
        type: boolean
jobs:
  setup-and-detect-changes:
    name: Setup & Change Detection
    runs-on: ubuntu-latest
    outputs:
      blog_changed: ${{ steps.change-detection.outputs.blog_changed }}
      topic_mode: ${{ steps.topic-mode.outputs.topic_mode }}
      topic_reason: ${{ steps.topic-mode.outputs.reason }}
      blog_hash: ${{ steps.blog-hash.outputs.hash }}
      requirements_hash: ${{ steps.requirements-hash.outputs.hash }}
    steps:
      - name: Extract Branch Name
        run: echo "BRANCH=$(echo ${GITHUB_REF##*/})" >> $GITHUB_ENV

      - name: Checkout Repository
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5
        with:
          fetch-depth: 0 # Need full history for change detection

      - name: Get Requirements Hash
        id: requirements-hash
        run: echo "hash=${{ hashFiles('website/scripts/python-requirements.txt') }}" >> $GITHUB_OUTPUT

      - name: Get Blog Content Hash
        id: blog-hash
        run: |
          # Create hash of blog content for topic model caching (excludes drafts)
          BLOG_HASH=$(find blog -name "*.md" -type f ! -path "*/drafts/*" -exec sha256sum {} \; | sha256sum | cut -d' ' -f1)
          echo "hash=${BLOG_HASH}" >> $GITHUB_OUTPUT
          echo "Blog content hash: ${BLOG_HASH}"

      - name: Detect File Changes
        id: change-detection
        run: |
          echo "Analyzing file changes to determine processing scope..."

          # Initialize defaults
          BLOG_CHANGED=false

          # Check manual workflow dispatch input
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "Manual workflow dispatch - will respect topic_mode input"
            BLOG_CHANGED=true  # Process content for manual runs
          fi

          # Check for scheduled run (weekly full discovery)
          if [ "${{ github.event_name }}" = "schedule" ]; then
            echo "Scheduled run - full processing required"
            BLOG_CHANGED=true
          fi

          # Check for blog content changes in push events
          if [ "${{ github.event_name }}" = "push" ] && [ "${{ github.event.before }}" != "0000000000000000000000000000000000000000" ]; then
            # Get list of all changed blog posts
            ALL_CHANGED_POSTS=$(git diff --name-only ${{ github.event.before }}..${{ github.sha }} | grep "^blog/.*\.md$" || true)
            
            # Filter out draft posts (posts in drafts/ folders or with published: false)
            CHANGED_POSTS=""
            if [ -n "$ALL_CHANGED_POSTS" ]; then
              echo "ðŸ” Analyzing blog post changes..."
              echo "$ALL_CHANGED_POSTS" | while read post; do
                echo "  ðŸ“„ Checking: $post"
              done
              
              # Filter out draft posts from processing
              CHANGED_POSTS=$(echo "$ALL_CHANGED_POSTS" | grep -v "/drafts/" || true)
              
              # Additional check: exclude posts marked as published: false
              if [ -n "$CHANGED_POSTS" ]; then
                PUBLISHABLE_POSTS=""
                for post in $CHANGED_POSTS; do
                  if [ -f "$post" ]; then
                    # Check if post has published: false in frontmatter
                    if grep -q "^published: false" "$post" 2>/dev/null; then
                      echo "  â¸ï¸  Skipping unpublished post: $post"
                    else
                      echo "  âœ… Including published post: $post"
                      PUBLISHABLE_POSTS="$PUBLISHABLE_POSTS$post"$'\n'
                    fi
                  fi
                done
                CHANGED_POSTS=$(echo "$PUBLISHABLE_POSTS" | grep -v '^$' || true)
              fi
            fi
            
            if [ -n "$CHANGED_POSTS" ]; then
              echo "âœ… Publishable blog content changes detected"
              BLOG_CHANGED=true
              
              # Count and display changed posts
              CHANGED_COUNT=$(echo "$CHANGED_POSTS" | grep -v '^$' | wc -l)
              echo "ðŸ“ Changed publishable blog posts ($CHANGED_COUNT):"
              echo "$CHANGED_POSTS" | while read post; do
                if [ -n "$post" ]; then
                  echo "  â€¢ $post"
                fi
              done
              
              # Save changed posts for granular processing
              echo "$CHANGED_POSTS" > /tmp/changed_blog_posts.txt
              
              # Set processing mode based on number of changes
              if [ "$CHANGED_COUNT" -le 5 ]; then
                echo "âœ¨ Small change set - enabling granular processing"
                echo "GRANULAR_MODE=true" >> /tmp/processing_mode.env
              else
                echo "ðŸ”„ Large change set - using full processing for reliability"
                echo "GRANULAR_MODE=false" >> /tmp/processing_mode.env
              fi
            else
              if [ -n "$ALL_CHANGED_POSTS" ]; then
                echo "ðŸ“ Blog file changes detected but all are drafts or unpublished posts:"
                echo "$ALL_CHANGED_POSTS" | while read post; do
                  echo "  â¸ï¸  $post"
                done
                echo "ðŸš€ Skipping metadata processing - no publishable content changes"
              else
                echo "ðŸ“­ No blog content changes detected"
              fi
              BLOG_CHANGED=false
              echo "GRANULAR_MODE=false" >> /tmp/processing_mode.env
            fi
          elif [ "${{ github.event_name }}" = "push" ]; then
            # First commit - process content
            echo "Initial commit - processing content"
            BLOG_CHANGED=true
            echo "GRANULAR_MODE=false" >> /tmp/processing_mode.env
          fi

          echo "blog_changed=${BLOG_CHANGED}" >> $GITHUB_OUTPUT
          echo "ðŸ” Blog content processing required: ${BLOG_CHANGED}"

      - name: Check Topic Cache Validity
        id: topic-cache
        run: |
          # Calculate current blog content hash using same method as local script (excludes drafts)
          CURRENT_HASH=$(find blog -name "*.md" -type f ! -path "*/drafts/*" -exec sha256sum {} \; | sha256sum | cut -d' ' -f1)
          echo "current_hash=${CURRENT_HASH}" >> $GITHUB_OUTPUT
          echo "ðŸ“Š Current blog content hash: ${CURRENT_HASH}"
          
          # This will be used to validate against cached topic models after cache restoration
          TOPICS_REGENERATION_NEEDED="unknown"
          echo "topics_regeneration_needed=${TOPICS_REGENERATION_NEEDED}" >> $GITHUB_OUTPUT

      - name: Determine Topic Extraction Mode
        id: topic-mode
        run: |
          # Initialize variables
          TOPIC_MODE="metadata-only"
          REASON="default fast mode"
          BLOG_CHANGED="${{ steps.change-detection.outputs.blog_changed }}"

          # Check manual workflow dispatch input
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            MANUAL_MODE="${{ inputs.topic_mode }}"
            if [ "$MANUAL_MODE" != "auto" ]; then
              TOPIC_MODE="$MANUAL_MODE"
              REASON="manual workflow dispatch: $MANUAL_MODE"
            fi
          fi

          # Check for scheduled run (weekly full discovery)
          if [ "${{ github.event_name }}" = "schedule" ]; then
            TOPIC_MODE="full-discovery"
            REASON="scheduled weekly full discovery"
          fi

          # Check commit message for manual triggers
          COMMIT_MSG="${{ github.event.head_commit.message }}"
          if [[ "$COMMIT_MSG" == *"[full-topics]"* ]]; then
            TOPIC_MODE="full-discovery"
            REASON="commit message contains [full-topics]"
          elif [[ "$COMMIT_MSG" == *"[force-topics]"* ]]; then
            TOPIC_MODE="force"
            REASON="commit message contains [force-topics]"
          fi

          # Auto-detect based on blog content changes
          if [ "$TOPIC_MODE" = "metadata-only" ] && [ "$BLOG_CHANGED" = "true" ]; then
            # Check if granular processing is enabled
            if [ -f "/tmp/processing_mode.env" ]; then
              source /tmp/processing_mode.env
              if [ "$GRANULAR_MODE" = "true" ]; then
                TOPIC_MODE="granular-processing"
                REASON="small blog changes - granular processing enabled"
              else
                TOPIC_MODE="full-discovery"
                REASON="blog content changes detected"
              fi
            else
              TOPIC_MODE="full-discovery"
              REASON="blog content changes detected"
            fi
          fi

          # Fast path optimization - if no blog changes, use cached metadata mode
          if [ "$BLOG_CHANGED" = "false" ] && [ "$TOPIC_MODE" = "metadata-only" ]; then
            TOPIC_MODE="use-cached"
            REASON="no blog changes - use cached metadata"
          fi

          echo "topic_mode=$TOPIC_MODE" >> $GITHUB_OUTPUT
          echo "reason=$REASON" >> $GITHUB_OUTPUT
          echo "ðŸŽ¯ Topic extraction mode: $TOPIC_MODE ($REASON)"

      - name: Upload Changed Posts Info
        if: steps.change-detection.outputs.blog_changed == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: changed-posts-info
          path: |
            /tmp/changed_blog_posts.txt
            /tmp/processing_mode.env
          retention-days: 1

  # Job for content processing - always runs to ensure metadata exists
  process-content:
    name: Process Blog Content
    runs-on: ubuntu-latest
    needs: setup-and-detect-changes
    steps:
      - name: Checkout Repository
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5

      - name: Download Changed Posts Info
        if: needs.setup-and-detect-changes.outputs.blog_changed == 'true'
        uses: actions/download-artifact@v5
        with:
          name: changed-posts-info
          path: /tmp/
        continue-on-error: true

      - name: Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.13"
          architecture: "x64"

      - name: Cache Python Virtual Environment
        uses: actions/cache@v4
        with:
          path: .venv
          key: python-venv-${{ runner.os }}-${{ needs.setup-and-detect-changes.outputs.requirements_hash }}-v2
          restore-keys: |
            python-venv-${{ runner.os }}-${{ needs.setup-and-detect-changes.outputs.requirements_hash }}-
            python-venv-${{ runner.os }}-

      - name: Cache Pip Wheel Cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-cache-${{ runner.os }}-${{ needs.setup-and-detect-changes.outputs.requirements_hash }}
          restore-keys: |
            pip-cache-${{ runner.os }}-

      - name: Cache NLTK Data
        uses: actions/cache@v4
        with:
          path: ~/nltk_data
          key: nltk-data-${{ runner.os }}-v3
          restore-keys: |
            nltk-data-${{ runner.os }}-v3-
            nltk-data-${{ runner.os }}-

      - name: Cache Hugging Face Transformers
        uses: actions/cache@v4
        with:
          path: ~/.cache/huggingface
          key: huggingface-transformers-${{ runner.os }}-${{ needs.setup-and-detect-changes.outputs.requirements_hash }}-v2
          restore-keys: |
            huggingface-transformers-${{ runner.os }}-${{ needs.setup-and-detect-changes.outputs.requirements_hash }}-
            huggingface-transformers-${{ runner.os }}-

      - name: Cache Topic Models
        uses: actions/cache@v4
        with:
          path: website/config/topic_models
          key: topic-models-${{ runner.os }}-${{ needs.setup-and-detect-changes.outputs.blog_hash }}-v4
          restore-keys: |
            topic-models-${{ runner.os }}-${{ needs.setup-and-detect-changes.outputs.blog_hash }}-v4-
            topic-models-${{ runner.os }}-

      - name: Validate Topic Cache Hash
        id: validate-cache
        run: |
          echo "ðŸ” Validating topic cache against current blog content..."
          
          # Get the current hash calculated earlier
          CURRENT_HASH="${{ needs.setup-and-detect-changes.outputs.blog_hash }}"
          
          # Check if we have cached topic models and hash file
          HASH_FILE="website/config/topic_models/blog_content_hash.txt"
          TOPICS_NEED_REGEN="true"
          CACHE_STATUS="no-cache"
          
          if [ -f "$HASH_FILE" ]; then
            CACHED_HASH=$(cat "$HASH_FILE" 2>/dev/null | tr -d '\n')
            echo "ðŸ“Š Current hash: ${CURRENT_HASH}"
            echo "ðŸ“ Cached hash:  ${CACHED_HASH}"
            
            if [ -n "$CACHED_HASH" ] && [ ${#CACHED_HASH} -eq 64 ]; then
              if [ "$CURRENT_HASH" = "$CACHED_HASH" ]; then
                echo "âœ… Blog content hash matches cached topics - no regeneration needed"
                TOPICS_NEED_REGEN="false"
                CACHE_STATUS="valid-cache"
              else
                echo "ðŸ”„ Blog content hash changed - topic regeneration needed"
                echo "   Previous: ${CACHED_HASH}"
                echo "   Current:  ${CURRENT_HASH}"
                TOPICS_NEED_REGEN="true" 
                CACHE_STATUS="invalid-cache"
              fi
            else
              echo "âš ï¸ Invalid cached hash (length: ${#CACHED_HASH}) - forcing regeneration"
              TOPICS_NEED_REGEN="true"
              CACHE_STATUS="corrupted-cache"
            fi
          else
            echo "ðŸ“ No cached hash file found - topic regeneration needed"
            TOPICS_NEED_REGEN="true"
            CACHE_STATUS="no-cache"
          fi
          
          echo "topics_need_regen=${TOPICS_NEED_REGEN}" >> $GITHUB_OUTPUT
          echo "cache_status=${CACHE_STATUS}" >> $GITHUB_OUTPUT
          echo "ðŸŽ¯ Topic regeneration needed: ${TOPICS_NEED_REGEN} (${CACHE_STATUS})"

      - name: Setup Topic Extraction System
        run: |
          chmod +x ./scripts/setup-topic-extraction.sh
          
          # Use hash-based validation instead of git diff for topic regeneration decision
          TOPICS_NEED_REGEN="${{ steps.validate-cache.outputs.topics_need_regen }}"
          CACHE_STATUS="${{ steps.validate-cache.outputs.cache_status }}"
          MODE="${{ needs.setup-and-detect-changes.outputs.topic_mode }}"
          
          echo "ðŸ” Hash-based validation: topics_need_regen=${TOPICS_NEED_REGEN}, cache_status=${CACHE_STATUS}"
          echo "ðŸ“‹ Workflow topic mode: ${MODE}"
          
          # Override topic mode based on hash validation for accuracy
          if [ "$TOPICS_NEED_REGEN" = "true" ]; then
            echo "ðŸ”„ Hash validation indicates topics need regeneration - running full discovery"
            ./scripts/setup-topic-extraction.sh
          else
            echo "âœ… Hash validation indicates topics are current - skipping discovery"
            ./scripts/setup-topic-extraction.sh --skip-discovery --no-metadata
          fi
        env:
          PYTHONIOENCODING: utf-8
          LC_ALL: C.UTF-8
          LANG: C.UTF-8
          TOKENIZERS_PARALLELISM: false
          HF_HOME: ~/.cache/huggingface
          PIP_CACHE_DIR: ~/.cache/pip
          PIP_DISABLE_PIP_VERSION_CHECK: 1
          PIP_NO_COMPILE: 1
          PIP_PREFER_BINARY: 1

      - name: Generate Blog Metadata with Smart Topic Extraction
        run: |
          chmod +x ./scripts/update-blog-metadata.sh

          # Use hash-based validation for metadata generation decision
          TOPICS_NEED_REGEN="${{ steps.validate-cache.outputs.topics_need_regen }}"
          CACHE_STATUS="${{ steps.validate-cache.outputs.cache_status }}"
          MODE="${{ needs.setup-and-detect-changes.outputs.topic_mode }}"
          
          echo "ðŸ” Hash validation for metadata: topics_need_regen=${TOPICS_NEED_REGEN}, cache_status=${CACHE_STATUS}"
          echo "ðŸ“‹ Original workflow mode: ${MODE}"
          
          # Use hash validation to determine the correct approach
          if [ "$MODE" = "use-cached" ]; then
            echo "ðŸš€ No publishable content changes - using existing cached metadata"
            ./scripts/update-blog-metadata.sh --use-cached-topics
          elif [ "$TOPICS_NEED_REGEN" = "false" ] && [ "$CACHE_STATUS" = "valid-cache" ]; then
            echo "âœ… Using cached topics for fast metadata generation (hash-validated)"
            ./scripts/update-blog-metadata.sh --use-cached-topics
          elif [ "$MODE" = "force" ]; then
            echo "âš¡ Running forced complete regeneration (manual override)..."
            ./scripts/update-blog-metadata.sh --force
          elif [ "$MODE" = "granular-processing" ] && [ -f "/tmp/changed_blog_posts.txt" ]; then
            echo "âš¡ Running granular processing for changed posts..."
            export CHANGED_POSTS_FILE="/tmp/changed_blog_posts.txt"
            ./scripts/update-blog-metadata.sh --incremental
          else
            echo "ðŸ”„ Running full metadata generation (topics regenerated or no cache)"
            ./scripts/update-blog-metadata.sh
          fi
        env:
          PYTHONIOENCODING: utf-8
          LC_ALL: C.UTF-8
          LANG: C.UTF-8

      - name: Verify Metadata Generation
        run: |
          if [ ! -f "website/public/blogdata/metadata/blog_metadata.json" ]; then
            echo "Error: Blog metadata file was not generated"
            exit 1
          fi
          echo "âœ… Blog metadata generated successfully"

      - name: Upload Blog Content and Metadata Artifact
        uses: actions/upload-artifact@v4
        with:
          name: blog-metadata
          path: |
            website/public/blogdata/
            website/config/topic_models/
          retention-days: 1

  # Parallel job for Node.js build
  build-site:
    name: Build Static Site
    runs-on: ubuntu-latest
    needs: [setup-and-detect-changes, process-content]
    if: always() && needs.process-content.result == 'success'
    steps:
      - name: Checkout Repository
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: latest
          cache: "npm"
          cache-dependency-path: website/package-lock.json

      - name: Cache Vite Build Cache
        uses: actions/cache@v4
        with:
          path: |
            website/node_modules/.vite
            website/node_modules/.cache
            website/.nuxt
            website/dist
          key: vite-cache-${{ runner.os }}-${{ hashFiles('website/package-lock.json', 'website/nuxt.config.ts') }}
          restore-keys: |
            vite-cache-${{ runner.os }}-${{ hashFiles('website/package-lock.json') }}-
            vite-cache-${{ runner.os }}-

      - name: Setup Python (for validation)
        uses: actions/setup-python@v6
        with:
          python-version: "3.13"

      - name: Download Blog Content and Metadata
        uses: actions/download-artifact@v5
        with:
          name: blog-metadata
          path: website/

      - name: Validate Blog Content and Metadata
        run: |
          echo "ðŸ” Validating blog content and metadata for build..."

          # Check if metadata was downloaded from artifact
          if [ -f "website/public/blogdata/metadata/blog_metadata.json" ]; then
            echo "âœ… Blog metadata found"

            # Validate metadata file is valid JSON and not empty
            if python -c "import json; data=json.load(open('website/public/blogdata/metadata/blog_metadata.json')); assert len(data) >= 0" 2>/dev/null; then
              METADATA_SIZE=$(wc -c < website/public/blogdata/metadata/blog_metadata.json)
              METADATA_POSTS=$(python -c "import json; data=json.load(open('website/public/blogdata/metadata/blog_metadata.json')); print(len(data))" 2>/dev/null || echo "0")
              echo "âœ… Metadata validated: ${METADATA_SIZE} bytes, ${METADATA_POSTS} posts"

              # Check if it's just empty array (minimal metadata)
              if [ "$METADATA_POSTS" = "0" ]; then
                echo "âš ï¸  Using minimal metadata - build will have limited blog functionality"
              fi
            else
              echo "âŒ Metadata file is invalid JSON - creating emergency fallback"
              echo "[]" > website/public/blogdata/metadata/blog_metadata.json
            fi
          else
            echo "âŒ Blog metadata not found - this should not happen!"
            echo "Creating emergency fallback metadata..."
            mkdir -p website/public/blogdata/metadata/
            echo "[]" > website/public/blogdata/metadata/blog_metadata.json
            echo "âš ï¸  Using empty metadata - build may have missing content"
          fi

          # Validate blog content files exist
          BLOG_FILE_COUNT=$(find website/public/blogdata -name "*.md" -type f | wc -l)
          if [ "$BLOG_FILE_COUNT" -gt 0 ]; then
            echo "âœ… Found $BLOG_FILE_COUNT blog content files"
          else
            echo "âš ï¸  No blog content files found - this may indicate an artifact transfer issue"
          fi

          # Final verification
          if [ ! -f "website/public/blogdata/metadata/blog_metadata.json" ]; then
            echo "âŒ FATAL: Could not ensure metadata file exists"
            exit 1
          else
            echo "âœ… Blog content and metadata validation complete - build can proceed"
          fi

      - name: Display Versions
        run: |
          echo "Node version: $(node --version)"
          echo "NPM version: $(npm --version)"

      - name: Install Node Dependencies
        working-directory: website
        run: npm ci

      - name: Get Highlight.js Stylesheets
        working-directory: website
        run: npm run getHighlightJsStyleSheets

      - name: Generate Build Cache Key
        id: build-cache-key
        run: |
          # Create comprehensive hash of all factors that affect build output (excludes drafts)
          BLOG_HASH=$(find blog -name "*.md" -type f ! -path "*/drafts/*" -exec sha256sum {} \; | sha256sum | cut -d' ' -f1)
          SOURCE_HASH=$(find website/app website/nuxt.config.ts website/package.json website/server -type f -exec sha256sum {} \; 2>/dev/null | sha256sum | cut -d' ' -f1)
          METADATA_HASH=$(sha256sum website/public/blogdata/metadata/blog_metadata.json | cut -d' ' -f1)
          # CONFIG_HASH - only include stable configuration files, exclude volatile binary and backup files
          CONFIG_HASH_FILES=$(find website/config -name "*.json" ! -name "*backup*" -type f 2>/dev/null | sort)
          if [ -n "$CONFIG_HASH_FILES" ]; then
            echo "ðŸ“‹ Config files for hash:"
            echo "$CONFIG_HASH_FILES" | while read f; do echo "  â€¢ $f"; done
            CONFIG_HASH=$(echo "$CONFIG_HASH_FILES" | xargs sha256sum 2>/dev/null | sha256sum | cut -d' ' -f1)
          else
            CONFIG_HASH="empty-config"
          fi
          ASSETS_HASH=$(find website/public/images website/public/styles -type f -exec sha256sum {} \; 2>/dev/null | sha256sum | cut -d' ' -f1)
          
          # Combine all hashes for comprehensive cache key
          COMBINED_HASH="${BLOG_HASH}-${SOURCE_HASH}-${METADATA_HASH}-${CONFIG_HASH}-${ASSETS_HASH}"
          CACHE_KEY="nuxt-build-${COMBINED_HASH}"
          
          echo "cache_key=${CACHE_KEY}" >> $GITHUB_OUTPUT
          echo "blog_hash=${BLOG_HASH}" >> $GITHUB_OUTPUT
          echo "source_hash=${SOURCE_HASH}" >> $GITHUB_OUTPUT
          
          # Debug: Show all hash components
          echo "ðŸ” Cache Key Components:"
          echo "  ðŸ“š BLOG_HASH:     ${BLOG_HASH}"
          echo "  ðŸ“¦ SOURCE_HASH:   ${SOURCE_HASH}"  
          echo "  ðŸ“„ METADATA_HASH: ${METADATA_HASH}"
          echo "  âš™ï¸  CONFIG_HASH:   ${CONFIG_HASH}"
          echo "  ðŸŽ¨ ASSETS_HASH:   ${ASSETS_HASH}"
          echo "ðŸ”‘ Build cache key: ${CACHE_KEY}"

      - name: Check Build Output Cache
        id: build-cache
        uses: actions/cache@v4
        with:
          path: |
            website/.output/public
            website/.output/nitro.json
          key: ${{ steps.build-cache-key.outputs.cache_key }}
          restore-keys: |
            nuxt-build-${{ steps.build-cache-key.outputs.blog_hash }}-${{ steps.build-cache-key.outputs.source_hash }}-
            nuxt-build-${{ steps.build-cache-key.outputs.blog_hash }}-
            nuxt-build-

      - name: Generate Static Site
        if: steps.build-cache.outputs.cache-hit != 'true'
        working-directory: website
        run: |
          echo "ðŸ—ï¸  Cache miss - generating static site..."
          echo "Build will take approximately 1.5-2 minutes..."
          
          # Enable Vite build caching optimizations
          export VITE_BUILD_TARGET=static
          export NODE_ENV=production
          
          # Run generation with timing
          START_TIME=$(date +%s)
          npm run generate
          END_TIME=$(date +%s)
          
          BUILD_TIME=$((END_TIME - START_TIME))
          echo "âš¡ Build completed in ${BUILD_TIME} seconds"

      - name: Verify Cached or Generated Build Output
        run: |
          if [ "${{ steps.build-cache.outputs.cache-hit }}" = "true" ]; then
            echo "âœ… Using cached build output - skipped generation"
            echo "Cache key: ${{ steps.build-cache-key.outputs.cache_key }}"
          else
            echo "âœ… Static site generated and cached for future builds"
          fi
          
          if [ ! -d "website/.output/public" ]; then
            echo "âŒ Error: Build output directory not found"
            exit 1
          fi
          
          # Display cache statistics
          BUILD_SIZE=$(du -sh website/.output/public | cut -f1)
          FILE_COUNT=$(find website/.output/public -type f | wc -l)
          echo "ðŸ“Š Build output: ${BUILD_SIZE} (${FILE_COUNT} files)"
          
          # Add to job summary
          {
            echo "## ðŸ—ï¸ Build Output Cache Performance"
            echo ""
            if [ "${{ steps.build-cache.outputs.cache-hit }}" = "true" ]; then
              echo "âœ… **Cache Hit** - Build skipped entirely"
              echo "- **Time Saved**: ~90-120 seconds"
              echo "- **Cache Key**: \`${{ steps.build-cache-key.outputs.cache_key }}\`"
            else
              echo "âŒ **Cache Miss** - Full build required"
              echo "- **Cache Key**: \`${{ steps.build-cache-key.outputs.cache_key }}\`"
              echo "- **Future builds** with same content will use cache"
            fi
            echo ""
            echo "### Build Statistics"
            echo "- **Output Size**: ${BUILD_SIZE}"
            echo "- **File Count**: ${FILE_COUNT} files"
            echo "- **Cached Paths**: \`.output/public\`, \`.output/nitro.json\`"
          } >> $GITHUB_STEP_SUMMARY


      - name: Upload Build Artifact
        uses: actions/upload-artifact@v4
        with:
          name: static-site
          path: website/.output/public/
          retention-days: 1

  # Final deployment job
  deploy:
    name: Deploy to GitHub Pages
    runs-on: ubuntu-latest
    needs: [setup-and-detect-changes, build-site]
    if: |
      always() &&
      needs.build-site.result == 'success' &&
      (
        (github.event_name == 'push' && github.ref_name == 'source') ||
        (github.event_name == 'schedule') ||
        (github.event_name == 'workflow_dispatch' && inputs.deploy == true)
      )
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - name: Download Build Artifact
        uses: actions/download-artifact@v5
        with:
          name: static-site
          path: ./public

      - name: Deploy to GitHub Pages
        id: deployment
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./public
          publish_branch: main
          keep_files: false
          force_orphan: true
          commit_message: |
            GitHub CI Updates [ci skip]

            Topic extraction mode: ${{ needs.setup-and-detect-changes.outputs.topic_mode }}
            Reason: ${{ needs.setup-and-detect-changes.outputs.topic_reason }}
            Triggered by: ${{ github.event_name }}
