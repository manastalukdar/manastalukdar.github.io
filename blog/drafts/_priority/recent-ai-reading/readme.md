---
published: false
tags:
 - Artificial Intelligence
 - Machine Learning
 - Papers I Read Recently Series
 - Reading
 - Generative AI
 - Synthetic Data
 - Data Generation
 - Synthetic Data Generation
 - Large Language Models
 - Model Training
 - Model Evaluation
 - LLM as Judge
 - Alignment with Human Feedback
 - Agentic AI
categories:
 - Technology
authors:
 - "Manas Talukdar"
post-format: link
title: Recent AI Reading [dd Month yyyy]
url-slug: recent-ai-reading-dd-month-yyyy
first-published-on: 2024-09-07 20:06
last-updated-on: 2024-09-07 20:06
meta:
 description: "Recent AI reading, including papers and articles."
excerpt: ""
---

# Recent AI Reading [dd Month yyyy]

## Papers

### Agentic AI

- [AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant](https://arxiv.org/abs/2410.18603)
  - [Source code](https://chengyou-jia.github.io/AgentStore-Home/) in [GitHub](https://github.com/chengyou-jia/AgentStore)
- [Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use](https://arxiv.org/abs/2410.24218)
  - [Teachable_RL source code](https://github.com/sled-group/Teachable_RL).

### AI Alignment with Human Feedback and Preferences, and other methods

- [Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form Text](https://arxiv.org/abs/2408.09235)
- [SelfCodeAlign: Self-Alignment for Code Generation](https://arxiv.org/abs/2410.24198)

### Model Evaluation

### Retrieval-Augmented Generation

### Synthetic Data

- [Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning](https://arxiv.org/abs/2410.19290)
- [Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent](https://arxiv.org/abs/2411.02265)
  - > Synthetic data is all you need? New large MoE from Tencent was trained on 1.5 trillion tokens of synthetic data. The 389B-A52B MoE outperforms @AIatMeta Llama 3.1 405B across academic benchmarks. [[ref](https://x.com/_philschmid/status/1853703814114623898)]

### Miscellaneous

- [Large Language Models Reflect the Ideology of their Creators](https://arxiv.org/abs/2410.18417)
- [Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!](https://arxiv.org/abs/2310.03693)
- [Datasets for Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2402.18041v1)
  - GitHub Repo: [Awesome-LLMs-Datasets](https://github.com/lmmlzn/Awesome-LLMs-Datasets)
- [Large Language Models and the Degradation of the Medical Record](https://www.nejm.org/doi/full/10.1056/NEJMp2405999)
- [DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation](https://arxiv.org/abs/2410.18666)
- [A Survey of Small Language Models](https://arxiv.org/abs/2410.20011)
- [Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction](https://arxiv.org/abs/2410.21169)
- [A Survey of Small Language Models](https://arxiv.org/abs/2410.20011)

## Technical Reports

## Articles and Blog Posts

- [Creating a LLM-as-a-Judge That Drives Business Results](https://hamel.dev/blog/posts/llm-judge/)
- [Understanding LLMs from Scratch Using Middle School Math](https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876)

## Miscellaneous

- [awesome-production-llm](https://github.com/jihoo-kim/awesome-production-llm) - A curated list of awesome open-source libraries for production LLM.
