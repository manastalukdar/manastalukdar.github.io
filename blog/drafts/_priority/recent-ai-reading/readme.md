---
published: false
tags:
 - Artificial Intelligence
 - Machine Learning
 - Papers I Read Recently Series
 - Reading
 - Generative AI
 - Synthetic Data
 - Data Generation
 - Synthetic Data Generation
 - Large Language Models
 - Model Training
 - Model Evaluation
 - LLM as Judge
 - Alignment with Human Feedback
categories:
 - Technology
authors:
 - "Manas Talukdar"
post-format: link
title: Recent AI Reading [dd Month yyyy]
url-slug: recent-ai-reading-dd-month-yyyy
first-published-on: 2024-09-07 20:06
last-updated-on: 2024-09-07 20:06
meta:
 description: "Recent AI reading, including papers and articles."
excerpt: ""
---

# Recent AI Reading [dd Month yyyy]

## Papers

- [Let's Verify Step by Step](https://arxiv.org/abs/2305.20050)
  - Blog post: [Improving mathematical reasoning with process supervision](https://openai.com/index/improving-mathematical-reasoning-with-process-supervision/)
  - Dataset: [GitHub](https://github.com/openai/prm800k?tab=readme-ov-file)
- [Training Language Models to Self-Correct via Reinforcement Learning](https://arxiv.org/abs/2409.12917)
- [The Perfect Blend: Redefining RLHF with Mixture of Judges](https://arxiv.org/abs/2409.20370)
- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
- [CoEdIT: Text Editing by Task-Specific Instruction Tuning](https://www.arxiv.org/abs/2305.09857)

### Synthetic Data

### Alignment with Human Feedback / Preferences

### Retrieval-Augmented Generation

- [A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge](https://arxiv.org/abs/2310.11703)

### Other

## Technical Reports

- [OpenAI o1 System Card](https://cdn.openai.com/o1-system-card.pdf)

## Articles and Blog Posts

- [Distance Metrics in Vector Search](https://weaviate.io/blog/distance-metrics-in-vector-search)
- [Introduction to Recurrent Neural Network](https://www.geeksforgeeks.org/introduction-to-recurrent-neural-network/)
- [Back Propagation in Recurrent Neural Networks](https://www.adityaagrawal.net/blog/deep_learning/bprop_rnn)
- [Embeddings in Machine Learning: Everything You Need to Know](https://www.featureform.com/post/the-definitive-guide-to-embeddings)
- [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)
- [A Gentle Introduction to Positional Encoding in Transformer Models, Part 1](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)
- [What is the intuition behind the dot product attention?](https://www.educative.io/answers/what-is-the-intuition-behind-the-dot-product-attention)
- [Transformers, Explained: Understand the Model Behind GPT-3, BERT, and T5](https://daleonai.com/transformers-explained)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Some Intuition on Attention and the Transformer](https://eugeneyan.com/writing/attention/)
- [How Transformers work in deep learning and NLP: an intuitive introduction](https://theaisummer.com/transformer/)

## Miscellaneous
