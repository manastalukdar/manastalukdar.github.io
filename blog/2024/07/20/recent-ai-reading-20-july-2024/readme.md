---
published: true
tags:
 - Artificial Intelligence
 - Machine Learning
 - Papers I Read Recently Series
 - Reading
 - Generative AI
 - Model Evaluation
 - Large Language Models
 - Model Training
categories:
 - Technology
authors:
 - "Manas Talukdar"
post-format: link
title: Recent AI Reading [20 July 2024]
url-slug: recent-ai-reading-20-july-2024
first-published-on: 2024-07-20 17:03
last-updated-on: 2024-07-20 17:03
series:
 slug: papers-i-read-recently
 part: 3
meta:
 description: "Recent AI reading, including papers and articles."
excerpt: "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language"
---

# Recent AI Reading [20 July 2024]

## Papers

- [LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models](https://arxiv.org/abs/2305.13711)
- [GenAI Arena: An Open Evaluation Platform for Generative Models](https://arxiv.org/abs/2406.04485) | [GenAI-Arena](https://huggingface.co/spaces/TIGER-Lab/GenAI-Arena)
- [Matching Anything by Segmenting Anything](https://arxiv.org/abs/2406.04221)
- [Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching](https://arxiv.org/abs/2406.06326)
- [Nemotron-4 340B Technical Report - Nvidia](https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T.pdf)
- [Creativity Has Left the Chat: The Price of Debiasing Language Models](https://arxiv.org/abs/2406.05587)
- [Rich Human Feedback for Text-to-Image Generation](https://arxiv.org/abs/2312.10240)
- [Threat Modelling and Risk Analysis for Large Language Model (LLM)-Powered Applications](https://arxiv.org/abs/2406.11007)
- [Prover-Verifier Games improve legibility of LLM outputs](https://arxiv.org/abs/2407.13692)
  - Corresponding article at OpenAI: [Prover-Verifier Games improve legibility of language model outputs](https://openai.com/index/prover-verifier-games-improve-legibility/)

## Articles and Blog Posts

- [Large Language Model Evaluation in 2024: 5 Methods](https://research.aimultiple.com/large-language-model-evaluation/)
- [Finding GPT-4â€™s mistakes with GPT-4](https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/)
- [A new initiative for developing third-party model evaluations](https://www.anthropic.com/news/a-new-initiative-for-developing-third-party-model-evaluations)
- [Can LLMs invent better ways to train LLMs?](https://sakana.ai/llm-squared/)
- [Notes on how to use LLMs in your product.](https://lethain.com/mental-model-for-how-to-use-llms-in-products/)

## Miscellaneous

- [MixEval - Deriving Wisdom of the Crowd from LLM Benchmark Mixtures](https://mixeval.github.io/)
- [LiveBench - A Challenging, Contamination-Free LLM Benchmark](https://livebench.ai)
- [Self-improving LLM Evals](https://docs.parea.ai/blog/self-improving-llm-evals)
