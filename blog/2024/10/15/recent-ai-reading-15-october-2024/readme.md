---
published: true
tags:
 - Artificial Intelligence
 - Machine Learning
 - Papers I Read Recently Series
 - Reading
 - Generative AI
 - Synthetic Data
 - Data Generation
 - Synthetic Data Generation
 - Large Language Models
 - Model Training
 - Model Evaluation
 - LLM as Judge
 - Alignment with Human Feedback
 - Agentic AI
 - Vision Language Models
categories:
 - Technology
authors:
 - "Manas Talukdar"
post-format: link
title: Recent AI Reading [15 October 2024]
url-slug: recent-ai-reading-15-october-2024
first-published-on: 2024-10-15 22:34
last-updated-on: 2024-10-15 22:34
meta:
 description: "Recent AI reading, including papers and articles."
excerpt: "Self-Boosting Large Language Models with Synthetic Preference Data"
---

# Recent AI Reading [15 October 2024]

## Papers

### Synthetic Data

- [Self-Boosting Large Language Models with Synthetic Preference Data](https://arxiv.org/abs/2410.06961)

### AI Alignment with Human Feedback / Preferences

- [The Perfect Blend: Redefining RLHF with Mixture of Judges](https://arxiv.org/abs/2409.20370)
- [RevisEval: Improving LLM-as-a-Judge via Response-Adapted References](https://arxiv.org/abs/2410.05193)
- [Better Instruction-Following Through Minimum Bayes Risk](https://arxiv.org/abs/2410.02902v1)
- [Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise](https://arxiv.org/abs/2410.03017)

### Retrieval-Augmented Generation

- [A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge](https://arxiv.org/abs/2310.11703)
- [Toward General Instruction-Following Alignment for Retrieval-Augmented Generation](https://arxiv.org/abs/2410.09584)

### Agentic AI

- [Agent S: An Open Agentic Framework that Uses Computers Like a Human](https://arxiv.org/abs/2410.08164)

### Other

- [Let's Verify Step by Step](https://arxiv.org/abs/2305.20050)
  - Blog post: [Improving mathematical reasoning with process supervision](https://openai.com/index/improving-mathematical-reasoning-with-process-supervision/)
  - Dataset: [GitHub](https://github.com/openai/prm800k?tab=readme-ov-file)
- [Training Language Models to Self-Correct via Reinforcement Learning](https://arxiv.org/abs/2409.12917)
- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
- [CoEdIT: Text Editing by Task-Specific Instruction Tuning](https://www.arxiv.org/abs/2305.09857)
- [Hyper-multi-step: The Truth Behind Difficult Long-context Tasks](https://arxiv.org/abs/2410.04422)
- [Were RNNs All We Needed?](https://arxiv.org/abs/2410.01201)
- [Intelligence at the Edge of Chaos](https://www.arxiv.org/abs/2410.02536)
- [RATIONALYST: Pre-training Process-Supervision for Improving Reasoning](https://arxiv.org/abs/2410.01044)
- [Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning](https://arxiv.org/abs/2410.06508)
- [From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning](https://arxiv.org/abs/2410.06456)
- [MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks](https://arxiv.org/abs/2410.10563)

## Technical Reports

- [OpenAI o1 System Card](https://cdn.openai.com/o1-system-card.pdf)

## Articles and Blog Posts

- [Distance Metrics in Vector Search](https://weaviate.io/blog/distance-metrics-in-vector-search)
- [Introduction to Recurrent Neural Network](https://www.geeksforgeeks.org/introduction-to-recurrent-neural-network/)
- [Back Propagation in Recurrent Neural Networks](https://www.adityaagrawal.net/blog/deep_learning/bprop_rnn)
- [Embeddings in Machine Learning: Everything You Need to Know](https://www.featureform.com/post/the-definitive-guide-to-embeddings)
- [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)
- [A Gentle Introduction to Positional Encoding in Transformer Models, Part 1](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)
- [What is the intuition behind the dot product attention?](https://www.educative.io/answers/what-is-the-intuition-behind-the-dot-product-attention)
- [Transformers, Explained: Understand the Model Behind GPT-3, BERT, and T5](https://daleonai.com/transformers-explained)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Some Intuition on Attention and the Transformer](https://eugeneyan.com/writing/attention/)
- [How Transformers work in deep learning and NLP: an intuitive introduction](https://theaisummer.com/transformer/)
- [AutoArena: An Open-Source AI Tool that Automates Head-to-Head Evaluations Using LLM Judges to Rank GenAI Systems](https://www.marktechpost.com/2024/10/09/autoarena-an-open-source-ai-tool-that-automates-head-to-head-evaluations-using-llm-judges-to-rank-genai-systems/)
