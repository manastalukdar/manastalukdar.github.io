---
published: true
tags:
 - Artificial Intelligence
 - Machine Learning
 - Papers I Read Recently Series
 - Reading
 - Generative AI
 - Synthetic Data
 - Data Generation
 - Synthetic Data Generation
 - Large Language Models
 - Model Training
 - LLM as Judge
 - Alignment with Human Feedback
 - Agentic AI
categories:
 - Technology
authors:
 - "Manas Talukdar"
post-format: link
title: Recent AI Reading [18 January 2025]
url-slug: recent-ai-reading-18-january-2025
first-published-on: 2025-01-18 20:44
last-updated-on: 2025-01-18 20:44
meta:
 description: "Recent AI reading, including papers and articles."
excerpt: ""
---

# Recent AI Reading [18 January 2025]

## Papers

### Agentic AI

- [A Multi-AI Agent System for Autonomous Optimization of Agentic AI Solutions via Iterative Refinement and LLM-Driven Feedback Loops](https://arxiv.org/abs/2412.17149)

### AI Alignment with Human Feedback and Preferences, and other methods

- [Offline Reinforcement Learning for LLM Multi-Step Reasoning](https://arxiv.org/abs/2412.16145)
- [RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning](https://arxiv.org/abs/2410.02089)
- [From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge](https://arxiv.org/abs/2411.16594)
  - <https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge>
  - <https://llm-as-a-judge.github.io>
- [Alignment faking in large language models](https://arxiv.org/abs/2412.14093)
  - <https://www.anthropic.com/research/alignment-faking>
  - <https://www.alignmentforum.org/posts/njAZwT8nkHnjipJku/alignment-faking-in-large-language-models>

### Retrieval-Augmented Generation

- [Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks](https://arxiv.org/abs/2412.15605v1)
- [GeAR: Graph-enhanced Agent for Retrieval-augmented Generation](https://arxiv.org/abs/2412.18431)

### Synthetic Data

- [Evaluating Language Models as Synthetic Data Generators](https://arxiv.org/abs/2412.03679)

### Miscellaneous

- [Training Large Language Models to Reason in a Continuous Latent Space](https://arxiv.org/abs/2412.06769)
- [Titans: Learning to Memorize at Test Time](https://arxiv.org/abs/2501.00663)
- [Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains](https://arxiv.org/abs/2501.05707v1)
- [Superhuman performance of a large language model on the reasoning tasks of a physician](https://arxiv.org/abs/2412.10849)
- [Byte Latent Transformer: Patches Scale Better Than Tokens](https://arxiv.org/abs/2412.09871)
- [OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models](https://arxiv.org/abs/2411.04905)
- [LLM Pruning and Distillation in Practice: The Minitron Approach](https://arxiv.org/abs/2408.11796)
- [Compact Language Models via Pruning and Knowledge Distillation](https://arxiv.org/abs/2407.14679)
- [Schrodinger's Memory: Large Language Models](https://arxiv.org/abs/2409.10482)
- [xLSTM: Extended Long Short-Term Memory](https://arxiv.org/abs/2405.04517) [xLSTMs]
- [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752) [Mamba SSM]
- [RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/abs/2305.13048) [RWKV]
- [Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models](https://arxiv.org/abs/2402.14207)
  - <https://storm.genie.stanford.edu>
- [BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack](https://arxiv.org/abs/2406.10149)
- [Model Collapse Demystified: The Case of Regression](https://arxiv.org/abs/2402.07712)

## Articles and Blog Posts

- [LLM-as-a-Judge vs Human Evaluation](https://www.galileo.ai/blog/llm-as-a-judge-vs-human-evaluation)
- [The Google Willow thing](https://scottaaronson.blog/?p=8525)

## Miscellaneous

- [Foundations of Large Language Models](https://arxiv.org/abs/2501.09223)
